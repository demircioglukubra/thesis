import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import shap
from shap import plots
from sklearn.impute import KNNImputer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import GroupKFold, RandomizedSearchCV
from sklearn.pipeline import Pipeline
from scipy.stats import uniform, randint
import joblib
import os
import xgboost as xgb
import lightgbm as lgb
from sklearn.ensemble import GradientBoostingRegressor

# ----------------------------------------------------------------------
# 1) LOAD & PREPROCESS
# ----------------------------------------------------------------------
data_path = r"C:\Users\demir\OneDrive\Desktop\MSc Thesis\lasstttrun\mad1_Ea.csv" #PATH HERE
df = pd.read_csv(data_path)
eps = 1e-6

df['devol_yield'] = df['devol_yield']/(100 - df['ac'])
df = df[(df['fuel_type'] != 'Torr-Wood')]

cat2_map = {
    'Digestate':             'Biomass',
    'Sewage':                'Mix',
    'HTC-MSW':               'Mix',
    'Cellulose':             'Biomass',
    'Lignin':                'Biomass',
    'Hemicellulose':         'Biomass',
    'Wood':                  'Biomass',
    'Torr-Wood':             'Biomass',
    'Lignite':               'Coal',
    'Torr-Coal':             'Coal',
    'Rubber':                'Plastic',
    'RDF1':                  'Plastic',
    'RDF2':                  'Mix',
    'Digestate_PP':          'Co-Pyro',
    'Digestate_SCP':         'Co-Pyro',
    'Digestate_PE':          'Co-Pyro'
}



therm_map = {
    'Digestate':             'Yes',
    'Sewage':                'No',
    'HTC-MSW':               'Yes',
    'Cellulose':             'No',
    'Lignin':                'Yes',
    'Hemicellulose':         'No',
    'Wood':                  'No',
    'Torr-Wood':             'Yes',
    'Lignite':               'No',
    'Torr-Coal':             'Yes',
    'Rubber':                'No',
    'RDF1':                  'No',
    'RDF2':                  'No',
    'Digestate_PP':          'No',
    'Digestate_SCP':         'No',
    'Digestate_PE':          'No'
}


df['fuel_category2']   = df['fuel_type'].map(cat2_map).fillna('Unknown')
df['thermal_treatment'] = df['fuel_type'].map(therm_map).fillna('Unknown')
# 3) One‐hot encode
df = pd.get_dummies(
    df,
    columns=['fuel_category2', 'thermal_treatment'],
    prefix=['fuel_cat2', 'th_treat'],
    drop_first=False
)

# ratio features
df['vm_fc']  = df['vm']         / (df['fc'] + eps)
df['oh_w'] = df['o'] / (df['h'] + eps)
df['ac_fc']  = df['ac']         / (df['fc'] + eps)
df['t_to_T'] = (df['temperature'] + 273.15) / df['heat_rate']
df['cl_ac']  = df['cl']         / (df['ac'] + eps)
df['n_ac']   = df['n']          / (df['ac'] + eps)

# ----------------------------------------------------------------------
# ——— BASIC LOG TRANSFORMS ———
for col in ['vm_fc', 'ac_fc', 'cl_ac', 'n_ac', 't_to_T','oc', 'hc', 'heat_rate', 'pressure']:
    df[f'log_{col}'] = np.log(df[col] + eps)


# ----------------------------------------------------------------------
# 2) STEP-1: Ea FIT WITH HYPERPARAMETER TUNING
# ----------------------------------------------------------------------
# define features to use for Ea fitting
ea_input_features = ['hc','oc', 'vm_fc', 'temperature', 'heat_rate',
                     'residence_time', 'pressure',
                      'ac_fc', 't_to_T',
                     'fuel_cat2_Biomass', 'fuel_cat2_Coal', 'fuel_cat2_Mix',
                     'fuel_cat2_Plastic', 'th_treat_No', 'th_treat_Yes']
X_ea = df[[col for col in ea_input_features if col in df.columns]]
y_ea = df['E_A']
groups = df['fuel_type']

# hyperparameter ranges for LGBM
param_space_ea = {
    "model__n_estimators":     randint(100, 500),
    "model__learning_rate":    uniform(0.01, 0.3),
    "model__num_leaves":       randint(20, 100),
    "model__max_depth":        randint(3, 10),
    "model__subsample":        uniform(0.6, 0.4),
    "model__colsample_bytree": uniform(0.6, 0.4),
    "model__min_child_samples":randint(5, 50),
}

# pipeline + randomized search
pipe_ea = Pipeline([
    ("scaler", StandardScaler()),
    ("model", lgb.LGBMRegressor(objective="huber", random_state=42))
])

gkf = GroupKFold(n_splits=len(groups.unique()))
search_ea = RandomizedSearchCV(
    pipe_ea,
    param_distributions=param_space_ea,
    n_iter=30,
    cv=gkf,
    scoring="neg_mean_squared_error",
    n_jobs=-1,
    random_state=42,
    verbose=1
)

# fit to find best Ea hyperparameters
search_ea.fit(X_ea, y_ea, groups=groups)
best_params_ea = search_ea.best_params_
print(">> Best Ea‐model params:", best_params_ea)

# leave-one-fuel-out evaluation of the tuned Ea model
ea_metrics = []
df['Ea_pred_tuned'] = np.nan

for fuel in groups.unique():
    tr = groups != fuel
    te = ~tr

    Xtr, ytr = X_ea[tr], y_ea[tr]
    Xte, yte = X_ea[te], y_ea[te]

    # scale
    scaler = StandardScaler().fit(Xtr)
    Xtr_s, Xte_s = scaler.transform(Xtr), scaler.transform(Xte)

    tuned_ea = lgb.LGBMRegressor(
        objective="huber",
        random_state=42,
        **{k.split("__",1)[1]: v for k, v in best_params_ea.items()}
    )
    tuned_ea.fit(Xtr_s, ytr)

    ypred = tuned_ea.predict(Xte_s)
    ea_metrics.append({
        "Fuel": fuel,
        "R2":   r2_score(yte, ypred),
        "MAE":  mean_absolute_error(yte, ypred),
        "MSE": mean_squared_error(yte, ypred),
    })

    df.loc[te, 'Ea_pred_tuned'] = ypred


scaler_ea_all = StandardScaler().fit(X_ea)
X_ea_all_scaled = scaler_ea_all.transform(X_ea)

final_ea_model = lgb.LGBMRegressor(
    objective="huber",
    random_state=42,
    **{k.split("__", 1)[1]: v for k, v in best_params_ea.items()}
)
final_ea_model.fit(X_ea_all_scaled, y_ea)

#joblib.dump((final_ea_model, scaler_ea_all), PATH HERE")

# display Ea tuning results
ea_results_df = pd.DataFrame(ea_metrics)
print("\nTuned Ea LOFO results:")
print(ea_results_df)

# use the tuned predictions going forward
df['EA_Arr'] = df['Ea_pred_tuned']

# ----------------------------------------------------------------------
# 2) STEP-1: Ea FIT WITH HYPERPARAMETER TUNING
# ----------------------------------------------------------------------
# define features to use for Ea fitting
# Define A0 input features explicitly and reorder
A0_input_features = [
    'hc','oc', 'vm_fc', 'temperature', 'heat_rate',
                     'residence_time', 'pressure',
                      'ac_fc', 't_to_T',
                     'fuel_cat2_Biomass', 'fuel_cat2_Coal', 'fuel_cat2_Mix',
                     'fuel_cat2_Plastic', 'th_treat_No', 'th_treat_Yes'
]

X_A = df[[col for col in A0_input_features if col in df.columns]]
y_A = df['A0']
groups = df['fuel_type']

# hyperparameter ranges for LGBM
param_space_A = {
    "model__n_estimators":     randint(100, 500),
    "model__learning_rate":    uniform(0.01, 0.3),
    "model__num_leaves":       randint(20, 100),
    "model__max_depth":        randint(3, 10),
    "model__subsample":        uniform(0.6, 0.4),
    "model__colsample_bytree": uniform(0.6, 0.4),
    "model__min_child_samples":randint(5, 50),
}

# pipeline + randomized search
pipe_A = Pipeline([
    ("scaler", StandardScaler()),
    ("model", lgb.LGBMRegressor(objective="huber", random_state=42))
])

gkf = GroupKFold(n_splits=len(groups.unique()))
search_A = RandomizedSearchCV(
    pipe_A,
    param_distributions=param_space_A,
    n_iter=30,
    cv=gkf,
    scoring="neg_mean_squared_error",
    n_jobs=-1,
    random_state=42,
    verbose=1
)

# fit to find best Ea hyperparameters
search_A.fit(X_A, y_A, groups=groups)
best_params_A = search_A.best_params_
print(">> Best A‐model params:", best_params_A)

# leave-one-fuel-out evaluation of the tuned Ea model
A_metrics = []
df['A_pred_tuned'] = np.nan

for fuel in groups.unique():
    tr = groups != fuel
    te = ~tr

    Xtr, ytr = X_A[tr], y_A[tr]
    Xte, yte = X_A[te], y_A[te]

    # scale
    scaler = StandardScaler().fit(Xtr)
    Xtr_s, Xte_s = scaler.transform(Xtr), scaler.transform(Xte)

    tuned_A = lgb.LGBMRegressor(
        objective="huber",
        random_state=42,
        **{k.split("__",1)[1]: v for k, v in best_params_A.items()}
    )
    tuned_A.fit(Xtr_s, ytr)

    # predict & log metrics
    ypred = tuned_A.predict(Xte_s)
    A_metrics.append({
        "Fuel": fuel,
        "R2":   r2_score(yte, ypred),
        "MAE":  mean_absolute_error(yte, ypred),
        "MSE": mean_squared_error(yte, ypred),
    })

    # store predictions back in df
    df.loc[te, 'A_pred_tuned'] = ypred

scaler_A_all = StandardScaler().fit(X_A)
X_A_all_scaled = scaler_A_all.transform(X_A)

# 2. Train model with best found hyperparameters
final_A_model = lgb.LGBMRegressor(
    objective="huber",
    random_state=42,
    **{k.split("__", 1)[1]: v for k, v in best_params_A.items()}
)
final_A_model.fit(X_A_all_scaled, y_A)

#joblib.dump((final_A_model, scaler_A_all), PATH HERE")

# display Ea tuning results
A_results_df = pd.DataFrame(A_metrics)
print("\nTuned Ea LOFO results:")
print(A_results_df)

# use the tuned predictions going forward
df['A_Arr'] = df['A_pred_tuned']

# ----------------------------------------------------------------------
# 3) PREPARE FOR STEP-2
# ----------------------------------------------------------------------
second = df.reset_index(drop=True)
groups = second['fuel_type']

drop2 = [
    'fuel_type','fuel_category','Unnamed: 0', 'E_A', 'A0', 'fuel_category2', 'thermal_treatment', 'A_pred_tuned', 'Ea_pred_tuned'
]
clean = second.drop(columns=drop2, errors='ignore').reset_index(drop=True)
clean['devol_yield'] = second['devol_yield']

# ----------------------------------------------------------------------
# 4) DEFINE MODELS & HYPERPARAMETER SPACES
# ----------------------------------------------------------------------
models = {
    "GB":  GradientBoostingRegressor,
    "XGB": xgb.XGBRegressor,
    "LGB": lgb.LGBMRegressor
}
param_space = {
    "GB": {
        "model__n_estimators":  randint(100,500),
        "model__learning_rate": uniform(0.05,0.2),
        "model__max_depth":     randint(3,7),
        "model__subsample":     uniform(0.7,0.3),
    },
    "XGB": {
        "model__n_estimators":     randint(100,500),
        "model__learning_rate":    uniform(0.05,0.2),
        "model__max_depth":        randint(3,7),
        "model__subsample":        uniform(0.7,0.3),
        "model__colsample_bytree": uniform(0.7,0.3),
    },
    "LGB": {
        "model__n_estimators":     randint(100,500),
        "model__learning_rate":    uniform(0.05,0.2),
        "model__num_leaves":       randint(20,80),
        "model__subsample":        uniform(0.7,0.3),
        "model__colsample_bytree": uniform(0.7,0.3),
    }
}

# ----------------------------------------------------------------------
# 5) DEFINE FEATURE-ENGINEERING CONFIGURATIONS
# ----------------------------------------------------------------------
# feature set definitions
baseline_feats = ['hc','oc', 'vm_fc' ,'temperature', 'heat_rate', 'residence_time', 'pressure']
baseline_Ea_feats = baseline_feats + ['EA_Arr']
baseline_Ea_A_feats = baseline_Ea_feats + ['A_Arr']
full_Ea_A = ['c', 'h', 'o', 'vm', 'fc','ac', 'n', 's' ,'cl', 'lhv',
           'hc','oc', 'temperature', 'pressure', 'heat_rate', 'residence_time', 'EA_Arr', 'A_Arr']
full_Ea = ['c', 'h', 'o', 'vm', 'fc','ac', 'n', 's' ,'cl', 'lhv',
           'hc','oc', 'temperature', 'pressure', 'heat_rate', 'residence_time', 'EA_Arr']
full = [f for f in full_Ea if f not in ['EA_Arr', 'A_Arr']]
ratio_feats    = ['vm_fc','t_to_T', 'ac_fc' ,'hc', 'oc','temperature', 'heat_rate', 'residence_time', 'pressure' ]
ratio_feats_Ea = ratio_feats + ['EA_Arr']
ratio_feats_Ea_A = ratio_feats_Ea + ['A_Arr']
operational_feats = ['temperature', 'heat_rate', 'residence_time', 'pressure']
ratio_feats_log = [f'log_{col}' for col in ['vm_fc', 'ac_fc', 'cl_ac', 'n_ac', 't_to_T','oc', 'hc', 'heat_rate', 'pressure']] + ['temperature', 'residence_time']
ratio_feats_log_Ea = ratio_feats_log + ['EA_Arr']
ratio_feats_log_Ea_A = ratio_feats_log_Ea + ['A_Arr']
ohe_feats2 = [c for c in clean.columns if c.startswith('th_treat')]
ohe_feats3 = [c for c in clean.columns if c.startswith('fuel_cat2')]


feature_sets = {
    "baseline":         baseline_feats,
    "baseline_ohe":     baseline_feats  + ohe_feats2 +ohe_feats3 ,
    "baseline_Ea":      baseline_Ea_feats,
    "baseline_Ea_ohe":  baseline_Ea_feats + ohe_feats2 +ohe_feats3 ,
    "baseline_Ea_A":    baseline_Ea_A_feats,
    "baseline_Ea_A_ohe": baseline_Ea_A_feats  + ohe_feats2 +ohe_feats3 ,
    "ratios":           ratio_feats,
    "ratios_ohe":       ratio_feats  + ohe_feats2 +ohe_feats3,
    "ratios_Ea":        ratio_feats_Ea,
    "ratios_Ea_ohe":    ratio_feats_Ea + ohe_feats2 +ohe_feats3 ,
    "ratios_Ea_A"  : ratio_feats_Ea_A,
    "ratios_Ea_A_ohe"  : ratio_feats_Ea_A  + ohe_feats2 +ohe_feats3,
    "full":             full,
    "full_Ea":          full_Ea,
    "full_Ea_ohe":      full_Ea  + ohe_feats2 +ohe_feats3 ,
    "full_Ea_A": full_Ea_A,
    "full_Ea_A_ohe": full_Ea_A+ ohe_feats2 +ohe_feats3,
    "full_ohe":         full + ohe_feats2 +ohe_feats3,
    "ratio_log":        ratio_feats_log,
    "ratio_log_Ea":     ratio_feats_log_Ea,
    "ratio_log_Ea_ohe": ratio_feats_log_Ea + ohe_feats2 +ohe_feats3,
    "ratio_log_Ea_A" : ratio_feats_log_Ea_A,
    "ratio_log_Ea_A_ohe" : ratio_feats_log_Ea_A + ohe_feats2 +ohe_feats3,

}

# ----------------------------------------------------------------------
# 6) LOOP OVER FEATURE SETS → TUNE & EVALUATE
# ----------------------------------------------------------------------
all_results    = []
all_parity     = []
best_params_all = {}

for fs_name, feats in feature_sets.items():
    print(f"\n=== Feature set: {fs_name} ===")
    Xc = clean[feats]
    yc = clean['devol_yield']
    best_params = {}
    gkf = GroupKFold(n_splits=len(groups.unique()))
    for name, Est in models.items():
        pipe = Pipeline([("scaler", StandardScaler()), ("model", Est())])
        search = RandomizedSearchCV(
            pipe,
            param_distributions=param_space[name],
            n_iter=20,
            cv=gkf,
            scoring="neg_mean_squared_error",
            n_jobs=-1,
            random_state=42
        )
        search.fit(Xc, yc, groups=groups)
        best_params[name] = search.best_params_
        print(f"  {name} best params → {search.best_params_}")
    best_params_all[fs_name] = best_params
    for fuel in groups.unique():
        tr = groups != fuel
        te = ~tr
        Xtr, ytr = Xc[tr], yc[tr]
        Xte, yte = Xc[te], yc[te]

        scaler = StandardScaler().fit(Xtr)
        Xtr_s, Xte_s = scaler.transform(Xtr), scaler.transform(Xte)

        for name, Est in models.items():
            params = {k.split("__",1)[1]: v for k,v in best_params[name].items()}
            m = Est(**params)
            m.fit(Xtr_s, ytr)
            yp = m.predict(Xte_s)

            all_results.append({
                "FeatureSet": fs_name,
                "Fuel":       fuel,
                "Model":      name,
                "R2":         r2_score(yte, yp),
                "MAE":        mean_absolute_error(yte, yp),
                "RMSE":       np.sqrt(mean_squared_error(yte, yp)),
            })
            for t_val, p_val in zip(yte, yp):
                all_parity.append({
                    "FeatureSet": fs_name,
                    "Fuel":       fuel,
                    "Model":      name,
                    "True":       t_val,
                    "Predicted":  p_val
                })
