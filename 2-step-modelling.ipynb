{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Main Pipeline\n",
    "- 2 models were trained to predict the estimated E_A & A_0 via Arrhenius method. (LGBM only bc it was the best performer during coding experiments)\n",
    "- Then, the main model (volatile release prediction) was trained by including and excluding the predicted Arrhenius parameters\n",
    "\n",
    "The entire code could be converted into object-oriented via AI tools easily if working in this setting is more convenient for the user."
   ],
   "id": "7a7540ff1b6d3b70"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from shap import plots\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GroupKFold, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import uniform, randint\n",
    "import joblib\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1) LOAD & PREPROCESS\n",
    "# ----------------------------------------------------------------------\n",
    "data_path = r\"C:\\Users\\demir\\OneDrive\\Desktop\\MSc Thesis\\lasstttrun\\mad1_Ea.csv\" #PATH HERE (cleaned dataset (via MAD) & Arrhenius parameters included)\n",
    "df = pd.read_csv(data_path)\n",
    "eps = 1e-6\n",
    "\n",
    "df['devol_yield'] = df['devol_yield']/(100 - df['ac'])\n",
    "df = df[(df['fuel_type'] != 'Torr-Wood')]\n",
    "\n",
    "cat2_map = {\n",
    "    'Digestate':             'Biomass',\n",
    "    'Sewage':                'Mix',\n",
    "    'HTC-MSW':               'Mix',\n",
    "    'Cellulose':             'Biomass',\n",
    "    'Lignin':                'Biomass',\n",
    "    'Hemicellulose':         'Biomass',\n",
    "    'Wood':                  'Biomass',\n",
    "    'Torr-Wood':             'Biomass',\n",
    "    'Lignite':               'Coal',\n",
    "    'Torr-Coal':             'Coal',\n",
    "    'Rubber':                'Plastic',\n",
    "    'RDF1':                  'Plastic',\n",
    "    'RDF2':                  'Mix',\n",
    "    'Digestate_PP':          'Co-Pyro',\n",
    "    'Digestate_SCP':         'Co-Pyro',\n",
    "    'Digestate_PE':          'Co-Pyro'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "therm_map = {\n",
    "    'Digestate':             'Yes',\n",
    "    'Sewage':                'No',\n",
    "    'HTC-MSW':               'Yes',\n",
    "    'Cellulose':             'No',\n",
    "    'Lignin':                'Yes',\n",
    "    'Hemicellulose':         'No',\n",
    "    'Wood':                  'No',\n",
    "    'Torr-Wood':             'Yes',\n",
    "    'Lignite':               'No',\n",
    "    'Torr-Coal':             'Yes',\n",
    "    'Rubber':                'No',\n",
    "    'RDF1':                  'No',\n",
    "    'RDF2':                  'No',\n",
    "    'Digestate_PP':          'No',\n",
    "    'Digestate_SCP':         'No',\n",
    "    'Digestate_PE':          'No'\n",
    "}\n",
    "\n",
    "\n",
    "df['fuel_category2']   = df['fuel_type'].map(cat2_map).fillna('Unknown')\n",
    "df['thermal_treatment'] = df['fuel_type'].map(therm_map).fillna('Unknown')\n",
    "\n",
    "# One‐hot encode\n",
    "df = pd.get_dummies(\n",
    "    df,\n",
    "    columns=['fuel_category2', 'thermal_treatment'],\n",
    "    prefix=['fuel_cat2', 'th_treat'],\n",
    "    drop_first=False\n",
    ")\n",
    "\n",
    "# ratio features\n",
    "df['vm_fc']  = df['vm']         / (df['fc'] + eps)\n",
    "df['oh_w'] = df['o'] / (df['h'] + eps)\n",
    "df['ac_fc']  = df['ac']         / (df['fc'] + eps)\n",
    "df['t_to_T'] = (df['temperature'] + 273.15) / df['heat_rate']\n",
    "df['cl_ac']  = df['cl']         / (df['ac'] + eps)\n",
    "df['n_ac']   = df['n']          / (df['ac'] + eps)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ——— BASIC LOG TRANSFORMS ———\n",
    "for col in ['vm_fc', 'ac_fc', 'cl_ac', 'n_ac', 't_to_T','oc', 'hc', 'heat_rate', 'pressure']:\n",
    "    df[f'log_{col}'] = np.log(df[col] + eps)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# STEP-1: Ea FIT WITH HYPERPARAMETER TUNING\n",
    "# ----------------------------------------------------------------------\n",
    "# define features to use for Ea fitting\n",
    "ea_input_features = ['hc','oc', 'vm_fc', 'temperature', 'heat_rate',\n",
    "                     'residence_time', 'pressure',\n",
    "                      'ac_fc', 't_to_T',\n",
    "                     'fuel_cat2_Biomass', 'fuel_cat2_Coal', 'fuel_cat2_Mix',\n",
    "                     'fuel_cat2_Plastic', 'th_treat_No', 'th_treat_Yes']\n",
    "X_ea = df[[col for col in ea_input_features if col in df.columns]]\n",
    "y_ea = df['E_A']\n",
    "groups = df['fuel_type']\n",
    "\n",
    "# hyperparameter ranges for LGBM\n",
    "param_space_ea = {\n",
    "    \"model__n_estimators\":     randint(100, 500),\n",
    "    \"model__learning_rate\":    uniform(0.01, 0.3),\n",
    "    \"model__num_leaves\":       randint(20, 100),\n",
    "    \"model__max_depth\":        randint(3, 10),\n",
    "    \"model__subsample\":        uniform(0.6, 0.4),\n",
    "    \"model__colsample_bytree\": uniform(0.6, 0.4),\n",
    "    \"model__min_child_samples\":randint(5, 50),\n",
    "}\n",
    "\n",
    "# pipeline + randomized search\n",
    "pipe_ea = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", lgb.LGBMRegressor(objective=\"huber\", random_state=42))\n",
    "])\n",
    "\n",
    "gkf = GroupKFold(n_splits=len(groups.unique()))\n",
    "search_ea = RandomizedSearchCV(\n",
    "    pipe_ea,\n",
    "    param_distributions=param_space_ea,\n",
    "    n_iter=30,\n",
    "    cv=gkf,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# fit to find best Ea hyperparameters\n",
    "search_ea.fit(X_ea, y_ea, groups=groups)\n",
    "best_params_ea = search_ea.best_params_\n",
    "print(\">> Best Ea‐model params:\", best_params_ea)\n",
    "\n",
    "# leave-one-fuel-out evaluation of the tuned Ea model\n",
    "ea_metrics = []\n",
    "df['Ea_pred_tuned'] = np.nan\n",
    "\n",
    "for fuel in groups.unique():\n",
    "    tr = groups != fuel\n",
    "    te = ~tr\n",
    "\n",
    "    Xtr, ytr = X_ea[tr], y_ea[tr]\n",
    "    Xte, yte = X_ea[te], y_ea[te]\n",
    "\n",
    "    # scale\n",
    "    scaler = StandardScaler().fit(Xtr)\n",
    "    Xtr_s, Xte_s = scaler.transform(Xtr), scaler.transform(Xte)\n",
    "\n",
    "    tuned_ea = lgb.LGBMRegressor(\n",
    "        objective=\"huber\",\n",
    "        random_state=42,\n",
    "        **{k.split(\"__\",1)[1]: v for k, v in best_params_ea.items()}\n",
    "    )\n",
    "    tuned_ea.fit(Xtr_s, ytr)\n",
    "\n",
    "    ypred = tuned_ea.predict(Xte_s)\n",
    "    ea_metrics.append({\n",
    "        \"Fuel\": fuel,\n",
    "        \"R2\":   r2_score(yte, ypred),\n",
    "        \"MAE\":  mean_absolute_error(yte, ypred),\n",
    "        \"MSE\": mean_squared_error(yte, ypred),\n",
    "    })\n",
    "\n",
    "    df.loc[te, 'Ea_pred_tuned'] = ypred\n",
    "\n",
    "\n",
    "scaler_ea_all = StandardScaler().fit(X_ea)\n",
    "X_ea_all_scaled = scaler_ea_all.transform(X_ea)\n",
    "\n",
    "final_ea_model = lgb.LGBMRegressor(\n",
    "    objective=\"huber\",\n",
    "    random_state=42,\n",
    "    **{k.split(\"__\", 1)[1]: v for k, v in best_params_ea.items()}\n",
    ")\n",
    "final_ea_model.fit(X_ea_all_scaled, y_ea)\n",
    "\n",
    "#joblib.dump((final_ea_model, scaler_ea_all), PATH HERE\")\n",
    "\n",
    "# display Ea tuning results\n",
    "ea_results_df = pd.DataFrame(ea_metrics)\n",
    "print(\"\\nTuned Ea LOFO results:\")\n",
    "print(ea_results_df)\n",
    "\n",
    "# use the tuned predictions going forward\n",
    "df['EA_Arr'] = df['Ea_pred_tuned']\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2) STEP-1: A0 FIT WITH HYPERPARAMETER TUNING\n",
    "# ----------------------------------------------------------------------\n",
    "# define features to use for A0 fitting\n",
    "A0_input_features = [\n",
    "    'hc','oc', 'vm_fc', 'temperature', 'heat_rate',\n",
    "                     'residence_time', 'pressure',\n",
    "                      'ac_fc', 't_to_T',\n",
    "                     'fuel_cat2_Biomass', 'fuel_cat2_Coal', 'fuel_cat2_Mix',\n",
    "                     'fuel_cat2_Plastic', 'th_treat_No', 'th_treat_Yes'\n",
    "]\n",
    "\n",
    "X_A = df[[col for col in A0_input_features if col in df.columns]]\n",
    "y_A = df['A0']\n",
    "groups = df['fuel_type']\n",
    "\n",
    "# hyperparameter ranges for LGBM\n",
    "param_space_A = {\n",
    "    \"model__n_estimators\":     randint(100, 500),\n",
    "    \"model__learning_rate\":    uniform(0.01, 0.3),\n",
    "    \"model__num_leaves\":       randint(20, 100),\n",
    "    \"model__max_depth\":        randint(3, 10),\n",
    "    \"model__subsample\":        uniform(0.6, 0.4),\n",
    "    \"model__colsample_bytree\": uniform(0.6, 0.4),\n",
    "    \"model__min_child_samples\":randint(5, 50),\n",
    "}\n",
    "\n",
    "# pipeline + randomized search\n",
    "pipe_A = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", lgb.LGBMRegressor(objective=\"huber\", random_state=42))\n",
    "])\n",
    "\n",
    "gkf = GroupKFold(n_splits=len(groups.unique()))\n",
    "search_A = RandomizedSearchCV(\n",
    "    pipe_A,\n",
    "    param_distributions=param_space_A,\n",
    "    n_iter=30,\n",
    "    cv=gkf,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# fit to find best Ea hyperparameters\n",
    "search_A.fit(X_A, y_A, groups=groups)\n",
    "best_params_A = search_A.best_params_\n",
    "print(\">> Best A‐model params:\", best_params_A)\n",
    "\n",
    "# leave-one-fuel-out evaluation of the tuned Ea model\n",
    "A_metrics = []\n",
    "df['A_pred_tuned'] = np.nan\n",
    "\n",
    "for fuel in groups.unique():\n",
    "    tr = groups != fuel\n",
    "    te = ~tr\n",
    "\n",
    "    Xtr, ytr = X_A[tr], y_A[tr]\n",
    "    Xte, yte = X_A[te], y_A[te]\n",
    "\n",
    "    # scale\n",
    "    scaler = StandardScaler().fit(Xtr)\n",
    "    Xtr_s, Xte_s = scaler.transform(Xtr), scaler.transform(Xte)\n",
    "\n",
    "    tuned_A = lgb.LGBMRegressor(\n",
    "        objective=\"huber\",\n",
    "        random_state=42,\n",
    "        **{k.split(\"__\",1)[1]: v for k, v in best_params_A.items()}\n",
    "    )\n",
    "    tuned_A.fit(Xtr_s, ytr)\n",
    "\n",
    "    # predict & log metrics\n",
    "    ypred = tuned_A.predict(Xte_s)\n",
    "    A_metrics.append({\n",
    "        \"Fuel\": fuel,\n",
    "        \"R2\":   r2_score(yte, ypred),\n",
    "        \"MAE\":  mean_absolute_error(yte, ypred),\n",
    "        \"MSE\": mean_squared_error(yte, ypred),\n",
    "    })\n",
    "\n",
    "    # store predictions back in df\n",
    "    df.loc[te, 'A_pred_tuned'] = ypred\n",
    "\n",
    "scaler_A_all = StandardScaler().fit(X_A)\n",
    "X_A_all_scaled = scaler_A_all.transform(X_A)\n",
    "\n",
    "# Train model with best found hyperparameters\n",
    "final_A_model = lgb.LGBMRegressor(\n",
    "    objective=\"huber\",\n",
    "    random_state=42,\n",
    "    **{k.split(\"__\", 1)[1]: v for k, v in best_params_A.items()}\n",
    ")\n",
    "final_A_model.fit(X_A_all_scaled, y_A)\n",
    "\n",
    "#joblib.dump((final_A_model, scaler_A_all), PATH HERE\")\n",
    "\n",
    "# display Ea tuning results\n",
    "A_results_df = pd.DataFrame(A_metrics)\n",
    "print(\"\\nTuned Ea LOFO results:\")\n",
    "print(A_results_df)\n",
    "\n",
    "# use the tuned predictions going forward\n",
    "df['A_Arr'] = df['A_pred_tuned']\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3) PREPARE FOR STEP-2\n",
    "# ----------------------------------------------------------------------\n",
    "second = df.reset_index(drop=True)\n",
    "groups = second['fuel_type']\n",
    "\n",
    "drop2 = [\n",
    "    'fuel_type','fuel_category','Unnamed: 0', 'E_A', 'A0', 'fuel_category2', 'thermal_treatment', 'A_pred_tuned', 'Ea_pred_tuned'\n",
    "]\n",
    "clean = second.drop(columns=drop2, errors='ignore').reset_index(drop=True)\n",
    "clean['devol_yield'] = second['devol_yield']\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4) DEFINE MODELS & HYPERPARAMETER SPACES\n",
    "# ----------------------------------------------------------------------\n",
    "models = {\n",
    "    \"GB\":  GradientBoostingRegressor,\n",
    "    \"XGB\": xgb.XGBRegressor,\n",
    "    \"LGB\": lgb.LGBMRegressor\n",
    "}\n",
    "param_space = {\n",
    "    \"GB\": {\n",
    "        \"model__n_estimators\":  randint(100,500),\n",
    "        \"model__learning_rate\": uniform(0.05,0.2),\n",
    "        \"model__max_depth\":     randint(3,7),\n",
    "        \"model__subsample\":     uniform(0.7,0.3),\n",
    "    },\n",
    "    \"XGB\": {\n",
    "        \"model__n_estimators\":     randint(100,500),\n",
    "        \"model__learning_rate\":    uniform(0.05,0.2),\n",
    "        \"model__max_depth\":        randint(3,7),\n",
    "        \"model__subsample\":        uniform(0.7,0.3),\n",
    "        \"model__colsample_bytree\": uniform(0.7,0.3),\n",
    "    },\n",
    "    \"LGB\": {\n",
    "        \"model__n_estimators\":     randint(100,500),\n",
    "        \"model__learning_rate\":    uniform(0.05,0.2),\n",
    "        \"model__num_leaves\":       randint(20,80),\n",
    "        \"model__subsample\":        uniform(0.7,0.3),\n",
    "        \"model__colsample_bytree\": uniform(0.7,0.3),\n",
    "    }\n",
    "}\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 5) DEFINE FEATURE-ENGINEERING CONFIGURATIONS\n",
    "# ----------------------------------------------------------------------\n",
    "# feature set definitions\n",
    "baseline_feats = ['hc','oc', 'vm_fc' ,'temperature', 'heat_rate', 'residence_time', 'pressure']\n",
    "baseline_Ea_feats = baseline_feats + ['EA_Arr']\n",
    "baseline_Ea_A_feats = baseline_Ea_feats + ['A_Arr']\n",
    "full_Ea_A = ['c', 'h', 'o', 'vm', 'fc','ac', 'n', 's' ,'cl', 'lhv',\n",
    "           'hc','oc', 'temperature', 'pressure', 'heat_rate', 'residence_time', 'EA_Arr', 'A_Arr']\n",
    "full_Ea = ['c', 'h', 'o', 'vm', 'fc','ac', 'n', 's' ,'cl', 'lhv',\n",
    "           'hc','oc', 'temperature', 'pressure', 'heat_rate', 'residence_time', 'EA_Arr']\n",
    "full = [f for f in full_Ea if f not in ['EA_Arr', 'A_Arr']]\n",
    "ratio_feats    = ['vm_fc','t_to_T', 'ac_fc' ,'hc', 'oc','temperature', 'heat_rate', 'residence_time', 'pressure' ]\n",
    "ratio_feats_Ea = ratio_feats + ['EA_Arr']\n",
    "ratio_feats_Ea_A = ratio_feats_Ea + ['A_Arr']\n",
    "operational_feats = ['temperature', 'heat_rate', 'residence_time', 'pressure']\n",
    "ratio_feats_log = [f'log_{col}' for col in ['vm_fc', 'ac_fc', 'cl_ac', 'n_ac', 't_to_T','oc', 'hc', 'heat_rate', 'pressure']] + ['temperature', 'residence_time']\n",
    "ratio_feats_log_Ea = ratio_feats_log + ['EA_Arr']\n",
    "ratio_feats_log_Ea_A = ratio_feats_log_Ea + ['A_Arr']\n",
    "ohe_feats2 = [c for c in clean.columns if c.startswith('th_treat')]\n",
    "ohe_feats3 = [c for c in clean.columns if c.startswith('fuel_cat2')]\n",
    "\n",
    "\n",
    "feature_sets = {\n",
    "    \"baseline\":         baseline_feats,\n",
    "    \"baseline_ohe\":     baseline_feats  + ohe_feats2 +ohe_feats3 ,\n",
    "    \"baseline_Ea\":      baseline_Ea_feats,\n",
    "    \"baseline_Ea_ohe\":  baseline_Ea_feats + ohe_feats2 +ohe_feats3 ,\n",
    "    \"baseline_Ea_A\":    baseline_Ea_A_feats,\n",
    "    \"baseline_Ea_A_ohe\": baseline_Ea_A_feats  + ohe_feats2 +ohe_feats3 ,\n",
    "    \"ratios\":           ratio_feats,\n",
    "    \"ratios_ohe\":       ratio_feats  + ohe_feats2 +ohe_feats3,\n",
    "    \"ratios_Ea\":        ratio_feats_Ea,\n",
    "    \"ratios_Ea_ohe\":    ratio_feats_Ea + ohe_feats2 +ohe_feats3 ,\n",
    "    \"ratios_Ea_A\"  : ratio_feats_Ea_A,\n",
    "    \"ratios_Ea_A_ohe\"  : ratio_feats_Ea_A  + ohe_feats2 +ohe_feats3,\n",
    "    \"full\":             full,\n",
    "    \"full_Ea\":          full_Ea,\n",
    "    \"full_Ea_ohe\":      full_Ea  + ohe_feats2 +ohe_feats3 ,\n",
    "    \"full_Ea_A\": full_Ea_A,\n",
    "    \"full_Ea_A_ohe\": full_Ea_A+ ohe_feats2 +ohe_feats3,\n",
    "    \"full_ohe\":         full + ohe_feats2 +ohe_feats3,\n",
    "    \"ratio_log\":        ratio_feats_log,\n",
    "    \"ratio_log_Ea\":     ratio_feats_log_Ea,\n",
    "    \"ratio_log_Ea_ohe\": ratio_feats_log_Ea + ohe_feats2 +ohe_feats3,\n",
    "    \"ratio_log_Ea_A\" : ratio_feats_log_Ea_A,\n",
    "    \"ratio_log_Ea_A_ohe\" : ratio_feats_log_Ea_A + ohe_feats2 +ohe_feats3,\n",
    "\n",
    "}\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 6) LOOP OVER FEATURE SETS → TUNE & EVALUATE\n",
    "# ----------------------------------------------------------------------\n",
    "all_results    = []\n",
    "all_parity     = []\n",
    "best_params_all = {}\n",
    "\n",
    "for fs_name, feats in feature_sets.items():\n",
    "    print(f\"\\n=== Feature set: {fs_name} ===\")\n",
    "    Xc = clean[feats]\n",
    "    yc = clean['devol_yield']\n",
    "    best_params = {}\n",
    "    gkf = GroupKFold(n_splits=len(groups.unique()))\n",
    "    for name, Est in models.items():\n",
    "        pipe = Pipeline([(\"scaler\", StandardScaler()), (\"model\", Est())])\n",
    "        search = RandomizedSearchCV(\n",
    "            pipe,\n",
    "            param_distributions=param_space[name],\n",
    "            n_iter=20,\n",
    "            cv=gkf,\n",
    "            scoring=\"neg_mean_squared_error\",\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "        search.fit(Xc, yc, groups=groups)\n",
    "        best_params[name] = search.best_params_\n",
    "        print(f\"  {name} best params → {search.best_params_}\")\n",
    "    best_params_all[fs_name] = best_params\n",
    "    for fuel in groups.unique():\n",
    "        tr = groups != fuel\n",
    "        te = ~tr\n",
    "        Xtr, ytr = Xc[tr], yc[tr]\n",
    "        Xte, yte = Xc[te], yc[te]\n",
    "\n",
    "        scaler = StandardScaler().fit(Xtr)\n",
    "        Xtr_s, Xte_s = scaler.transform(Xtr), scaler.transform(Xte)\n",
    "\n",
    "        for name, Est in models.items():\n",
    "            params = {k.split(\"__\",1)[1]: v for k,v in best_params[name].items()}\n",
    "            m = Est(**params)\n",
    "            m.fit(Xtr_s, ytr)\n",
    "            yp = m.predict(Xte_s)\n",
    "\n",
    "            all_results.append({\n",
    "                \"FeatureSet\": fs_name,\n",
    "                \"Fuel\":       fuel,\n",
    "                \"Model\":      name,\n",
    "                \"R2\":         r2_score(yte, yp),\n",
    "                \"MAE\":        mean_absolute_error(yte, yp),\n",
    "                \"RMSE\":       np.sqrt(mean_squared_error(yte, yp)),\n",
    "            })\n",
    "            for t_val, p_val in zip(yte, yp):\n",
    "                all_parity.append({\n",
    "                    \"FeatureSet\": fs_name,\n",
    "                    \"Fuel\":       fuel,\n",
    "                    \"Model\":      name,\n",
    "                    \"True\":       t_val,\n",
    "                    \"Predicted\":  p_val\n",
    "                })\n"
   ],
   "id": "630c9d391c14abae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Best performers list",
   "id": "7ab92bc1797df9ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results_df = pd.DataFrame(all_results)\n",
    "parity_df  = pd.DataFrame(all_parity)\n",
    "\n",
    "print(\"\\nResults (first 10 rows):\")\n",
    "print(results_df.head(10))\n",
    "print(\"\\nParity (first 10 rows):\")\n",
    "print(parity_df.head(10))\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 8) IDENTIFY THE BEST MODEL FEATURE SET COMBINATION\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Calculate macro-averaged MAE across fuels\n",
    "avg_perf = (results_df.groupby(['FeatureSet', 'Model'])\n",
    "                     .agg(avg_R2=('R2', 'mean'),\n",
    "                          avg_MAE=('MAE', 'mean'),\n",
    "                          avg_RMSE=('RMSE', 'mean'))\n",
    "                     .reset_index())\n",
    "\n",
    "# Choose model with the lowest avg MAE (you can also prioritize highest avg R2)\n",
    "best_row = avg_perf.sort_values(by='avg_MAE').iloc[0]\n",
    "\n",
    "final_fs_name     = best_row['FeatureSet']\n",
    "final_model_name  = best_row['Model']\n",
    "final_feats       = feature_sets[final_fs_name]\n",
    "\n",
    "print(f\"\\nBest model: {final_model_name} with feature set: {final_fs_name}\")\n",
    "print(best_row)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 9) TRAIN FINAL MODEL ON FULL DATA (except validation)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Prepare full training data\n",
    "X_final = clean[final_feats]\n",
    "y_final = clean['devol_yield']\n",
    "scaler_final = StandardScaler().fit(X_final)\n",
    "X_final_scaled = scaler_final.transform(X_final)\n",
    "\n",
    "# best parameters for the final model\n",
    "best_params_final = {\n",
    "    k.split(\"__\", 1)[1]: v\n",
    "    for k, v in best_params_all[final_fs_name][final_model_name].items()\n",
    "}\n",
    "\n",
    "# Instantiate the model\n",
    "if final_model_name == \"XGB\":\n",
    "    final_model = xgb.XGBRegressor(random_state=42, **best_params_final)\n",
    "elif final_model_name == \"LGB\":\n",
    "    final_model = lgb.LGBMRegressor(random_state=42, **best_params_final)\n",
    "elif final_model_name == \"GB\":\n",
    "    final_model = GradientBoostingRegressor(random_state=42, **best_params_final)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model type: {final_model_name}\")\n",
    "\n",
    "# Fit and save\n",
    "final_model.fit(X_final_scaled, y_final)\n",
    "\n",
    "#joblib.dump((final_model, scaler_final),path)\n"
   ],
   "id": "aa2ae02480471f60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Filter for best performing algorithm (XGB) results only\n",
    "xgb_results = results_df[results_df['Model'] == 'XGB']\n",
    "\n",
    "# Compute average performance across fuels for each feature set (only for XGB)\n",
    "xgb_avg_perf = (xgb_results.groupby('FeatureSet')\n",
    "                            .agg(avg_R2=('R2', 'mean'),\n",
    "                                 avg_MAE=('MAE', 'mean'),\n",
    "                                 avg_RMSE=('RMSE', 'mean'))\n",
    "                            .reset_index())\n",
    "\n",
    "# Sort by MAE to find best-performing feature set for XGB\n",
    "best_xgb_row = xgb_avg_perf.sort_values(by='avg_MAE').iloc[1]\n",
    "\n",
    "final_fs_name     = best_xgb_row['FeatureSet']\n",
    "final_model_name  = 'XGB'\n",
    "final_feats       = feature_sets[final_fs_name]\n",
    "\n",
    "print(f\"\\nBest XGB feature set: {final_fs_name}\")\n",
    "print(best_xgb_row)\n"
   ],
   "id": "cfa40853f7cb62d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## SHAP Analysis",
   "id": "4424c25aa4105f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import shap\n",
    "\n",
    "X_final = clean[final_feats]\n",
    "y_final = clean['devol_yield']\n",
    "\n",
    "# Scale the features\n",
    "scaler_final = StandardScaler().fit(X_final)\n",
    "X_final_scaled = scaler_final.transform(X_final)\n",
    "X_final_scaled_df = pd.DataFrame(X_final_scaled, columns=X_final.columns)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2. Define prefixes of OHE features to exclude\n",
    "# ----------------------------------------------------------------------\n",
    "ohe_prefixes = ['fuel_cat2_', 'th_treat_']\n",
    "\n",
    "# Filter out OHE features\n",
    "non_ohe_features = [\n",
    "    col for col in X_final_scaled_df.columns\n",
    "    if not any(col.startswith(prefix) for prefix in ohe_prefixes)\n",
    "]\n",
    "\n",
    "# Get column indices for SHAP filtering\n",
    "non_ohe_indices = [X_final_scaled_df.columns.get_loc(col) for col in non_ohe_features]\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3. SHAP explanation using TreeExplainer\n",
    "# ----------------------------------------------------------------------\n",
    "explainer = shap.Explainer(final_model, X_final_scaled_df)\n",
    "shap_values = explainer(X_final_scaled_df)\n",
    "\n",
    "# Filter SHAP values to exclude OHE columns\n",
    "shap_values_filtered = shap_values[:, non_ohe_indices]\n",
    "X_filtered_df = X_final_scaled_df[non_ohe_features]\n",
    "\n",
    "shap.plots.beeswarm(shap_values_filtered, max_display=20)\n",
    "plt.tight_layout()\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ],
   "id": "68983c8ed58db639"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "metrics = ['R2', 'MAE', 'RMSE']\n",
    "feature_sets = results_df['FeatureSet'].unique()\n",
    "models_list  = results_df['Model'].unique()\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    data = []\n",
    "    labels = []\n",
    "    for fs in feature_sets:\n",
    "        for model in models_list:\n",
    "            vals = results_df[\n",
    "                (results_df['FeatureSet'] == fs) &\n",
    "                (results_df['Model'] == model)\n",
    "            ][metric]\n",
    "            data.append(vals)\n",
    "            labels.append(f\"{fs}-{model}\")\n",
    "    plt.boxplot(data, labels=labels)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title(f\"{metric} Distribution by FeatureSet & Model\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "fuels = parity_df['Fuel'].unique()\n",
    "\n",
    "perf_lookup = (\n",
    "    results_df\n",
    "    .set_index(['FeatureSet','Model','Fuel'])\n",
    "    [['R2','MAE','RMSE']]\n",
    ")"
   ],
   "id": "bc07fd317e019bb2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "perf_lookup = results_df.set_index(['FeatureSet','Model','Fuel'])[['R2','MAE','RMSE']]\n",
    "\n",
    "feature_sets = results_df['FeatureSet'].unique()\n",
    "models_list  = results_df['Model'].unique()\n",
    "fuels_list   = results_df['Fuel'].unique()\n",
    "\n",
    "cmap = plt.cm.get_cmap('tab20', len(fuels_list))\n",
    "fuel_colors = {fuel: cmap(i) for i, fuel in enumerate(fuels_list)}\n",
    "\n",
    "for fs in feature_sets:\n",
    "    for model in models_list:\n",
    "        sub = parity_df[(parity_df['FeatureSet']==fs) & (parity_df['Model']==model)]\n",
    "        if sub.empty:\n",
    "            continue\n",
    "\n",
    "        # grab best params for this (fs, model)\n",
    "        bp = best_params_all[fs][model]\n",
    "        param_str = \", \".join(f\"{k.split('__',1)[1]}={v}\" for k,v in bp.items())\n",
    "\n",
    "        plt.figure(figsize=(12,8))\n",
    "        for fuel in fuels_list:\n",
    "            sub2 = sub[sub['Fuel']==fuel]\n",
    "            if sub2.empty:\n",
    "                continue\n",
    "\n",
    "            r2, mae, rmse = perf_lookup.loc[(fs, model, fuel)]\n",
    "            label = f\"{fuel}\\nR²={r2:.2f}, MAE={mae:.2f}, RMSE={rmse:.2f}\"\n",
    "\n",
    "            plt.scatter(sub2['True'], sub2['Predicted'],\n",
    "                        label=label, alpha=0.6, s=60,\n",
    "                        color=fuel_colors[fuel])\n",
    "\n",
    "        mn, mx = sub[['True','Predicted']].values.min(), sub[['True','Predicted']].values.max()\n",
    "        plt.plot([mn,mx],[mn,mx], '--', color='gray')\n",
    "\n",
    "        plt.title(f\"{model} | {fs}\\nBest params: {param_str}\", fontsize=14)\n",
    "        plt.xlabel('True Volatile Yield', fontsize=14)\n",
    "        plt.ylabel('Predicted Volatile Yield', fontsize=14)\n",
    "        plt.legend(bbox_to_anchor=(1.05,1), loc='upper left', fontsize='large')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ],
   "id": "b1dccf13799dfcb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "metrics      = ['R2', 'MAE', 'RMSE']\n",
    "feature_sets = results_df['FeatureSet'].unique()\n",
    "models_list  = results_df['Model'].unique()\n",
    "\n",
    "N = len(feature_sets)\n",
    "angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "cmap = plt.get_cmap('tab10')\n",
    "model_colors = {model: cmap(i) for i, model in enumerate(models_list)}\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    1, 3,\n",
    "    figsize=(22, 8),\n",
    "    subplot_kw=dict(polar=True)\n",
    ")\n",
    "fig.suptitle('Model Comparison Across Feature Sets', fontsize=24, y=1)\n",
    "\n",
    "handles = []\n",
    "labels = []\n",
    "\n",
    "for ax, metric in zip(axes, metrics):\n",
    "    for model in models_list:\n",
    "        df_m = results_df[results_df['Model'] == model]\n",
    "        agg  = df_m.groupby('FeatureSet')[metric].mean()\n",
    "        values = [agg.get(fs, np.nan) for fs in feature_sets]\n",
    "        values += values[:1]\n",
    "\n",
    "        h, = ax.plot(angles, values,\n",
    "                     color=model_colors[model],\n",
    "                     linewidth=2,\n",
    "                     label=model)\n",
    "        ax.fill(angles, values,\n",
    "                color=model_colors[model],\n",
    "                alpha=0.15)\n",
    "\n",
    "        if metric == metrics[0]:\n",
    "            handles.append(h)\n",
    "            labels.append(model)\n",
    "\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(feature_sets, fontsize=12)\n",
    "    ax.set_title(f'Mean {metric}', pad=15, fontsize=18)\n",
    "\n",
    "    if metric == 'R2':\n",
    "        ax.set_ylim(0, 1)\n",
    "    else:\n",
    "        overall_max = results_df.groupby(['Model','FeatureSet'])[metric].mean().max()\n",
    "        ax.set_ylim(0, overall_max * 1.1)\n",
    "\n",
    "legend = fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    loc='upper center',\n",
    "    ncol=len(models_list),\n",
    "    bbox_to_anchor=(0.5, 0.02),\n",
    "    prop={'size': 18},\n",
    "    handletextpad=1.5,\n",
    "    handlelength=3,\n",
    "    columnspacing=2)\n",
    "\n",
    "legend.get_frame().set_linewidth(1.5)\n",
    "legend.get_frame().set_edgecolor('black')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "plt.show()\n"
   ],
   "id": "7b967abad1a2d286"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from itertools import compress\n",
    "\n",
    "all_cols      = results_df.columns.tolist()\n",
    "fixed_cols    = ['FeatureSet','Model','Fuel','R2','MAE','RMSE']\n",
    "hp_cols       = [c for c in all_cols if c not in fixed_cols]\n",
    "\n",
    "# Aggregate performance metrics across fuels\n",
    "agg = (\n",
    "    results_df\n",
    "    .groupby(['FeatureSet','Model'] + hp_cols)\n",
    "    .agg({\n",
    "        'R2':  'mean',\n",
    "        'MAE': 'mean',\n",
    "        'RMSE':'mean'\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Sort to get the “best” at the top\n",
    "best = agg.sort_values(\n",
    "    by=['R2','MAE','RMSE'],\n",
    "    ascending=[False, True, True]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"Top 10 hyper‐parameter combos (averaged over all fuels):\")\n",
    "print(best.head(10))\n",
    "\n",
    "is_dominated = [False]*len(best)\n",
    "for i, row_i in best.iterrows():\n",
    "    for j, row_j in best.iterrows():\n",
    "        if (row_j['R2']   >= row_i['R2'] and\n",
    "            row_j['MAE']  <= row_i['MAE'] and\n",
    "            row_j['RMSE'] <= row_i['RMSE']\n",
    "           ) and (j != i):\n",
    "\n",
    "            if ((row_j['R2']   > row_i['R2']) or\n",
    "                (row_j['MAE']  < row_i['MAE']) or\n",
    "                (row_j['RMSE'] < row_i['RMSE'])):\n",
    "                is_dominated[i] = True\n",
    "                break\n",
    "\n",
    "pareto = best.loc[list(compress(range(len(best)), [not d for d in is_dominated]))]\n",
    "print(\"\\nPareto‐optimal hyper‐parameter settings:\")\n",
    "print(pareto)\n"
   ],
   "id": "7b7bbbc72df8e3c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Varience Inflation Factor\n",
    "Change over feature sets"
   ],
   "id": "5f1a05acbbc91eb6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# VIF DETERMINATION ACROSS FEATURE SETS\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "baseline_feats = ['hc','oc', 'vm_fc' ,'temperature', 'heat_rate', 'residence_time', 'pressure']\n",
    "baseline_Ea_feats = baseline_feats + ['EA_Arr']\n",
    "baseline_Ea_A_feats = baseline_Ea_feats + ['A_Arr']\n",
    "full_Ea_A = ['c', 'h', 'o', 'vm', 'fc','ac', 'n', 's' ,'cl', 'lhv',\n",
    "           'hc','oc', 'temperature', 'pressure', 'heat_rate', 'residence_time', 'EA_Arr', 'A_Arr']\n",
    "full_Ea = ['c', 'h', 'o', 'vm', 'fc','ac', 'n', 's' ,'cl', 'lhv',\n",
    "           'hc','oc', 'temperature', 'pressure', 'heat_rate', 'residence_time', 'EA_Arr']\n",
    "full = [f for f in full_Ea if f not in ['EA_Arr', 'A_Arr']]\n",
    "ratio_feats    = ['vm_fc','t_to_T', 'ac_fc' ,'hc', 'oc','temperature', 'heat_rate', 'residence_time', 'pressure' ]\n",
    "ratio_feats_Ea = ratio_feats + ['EA_Arr']\n",
    "ratio_feats_Ea_A = ratio_feats_Ea + ['A_Arr']\n",
    "operational_feats = ['temperature', 'heat_rate', 'residence_time', 'pressure']\n",
    "ratio_feats_log = [f'log_{col}' for col in ['vm_fc', 'ac_fc', 'cl_ac', 'n_ac', 't_to_T','oc', 'hc', 'heat_rate', 'pressure']] + ['temperature', 'residence_time']\n",
    "ratio_feats_log_Ea = ratio_feats_log + ['EA_Arr']\n",
    "ratio_feats_log_Ea_A = ratio_feats_log_Ea + ['A_Arr']\n",
    "ohe_feats2 = [c for c in clean.columns if c.startswith('th_treat')]\n",
    "ohe_feats3 = [c for c in clean.columns if c.startswith('fuel_cat2')]\n",
    "\n",
    "\n",
    "feature_sets = {\n",
    "    \"baseline\":         baseline_feats,\n",
    "    \"baseline_ohe\":     baseline_feats  + ohe_feats2 +ohe_feats3 ,\n",
    "    \"baseline_Ea\":      baseline_Ea_feats,\n",
    "    \"baseline_Ea_ohe\":  baseline_Ea_feats + ohe_feats2 +ohe_feats3 ,\n",
    "    \"baseline_Ea_A\":    baseline_Ea_A_feats,\n",
    "    \"baseline_Ea_A_ohe\": baseline_Ea_A_feats  + ohe_feats2 +ohe_feats3 ,\n",
    "    \"ratios\":           ratio_feats,\n",
    "    \"ratios_ohe\":       ratio_feats  + ohe_feats2 +ohe_feats3,\n",
    "    \"ratios_Ea\":        ratio_feats_Ea,\n",
    "    \"ratios_Ea_ohe\":    ratio_feats_Ea + ohe_feats2 +ohe_feats3 ,\n",
    "    \"ratios_Ea_A\"  : ratio_feats_Ea_A,\n",
    "    \"ratios_Ea_A_ohe\"  : ratio_feats_Ea_A  + ohe_feats2 +ohe_feats3,\n",
    "    \"full\":             full,\n",
    "    \"full_Ea\":          full_Ea,\n",
    "    \"full_Ea_ohe\":      full_Ea  + ohe_feats2 +ohe_feats3 ,\n",
    "    \"full_Ea_A\": full_Ea_A,\n",
    "    \"full_Ea_A_ohe\": full_Ea_A+ ohe_feats2 +ohe_feats3,\n",
    "    \"full_ohe\":         full + ohe_feats2 +ohe_feats3,\n",
    "    \"ratio_log\":        ratio_feats_log,\n",
    "    \"ratio_log_Ea\":     ratio_feats_log_Ea,\n",
    "    \"ratio_log_Ea_ohe\": ratio_feats_log_Ea + ohe_feats2 +ohe_feats3,\n",
    "    \"ratio_log_Ea_A\" : ratio_feats_log_Ea_A,\n",
    "    \"ratio_log_Ea_A_ohe\" : ratio_feats_log_Ea_A + ohe_feats2 +ohe_feats3,\n",
    "\n",
    "}\n",
    "\n",
    "for fs_name, feats in feature_sets.items():\n",
    "    df_sub = clean[feats].dropna()\n",
    "    df_sub = df_sub.select_dtypes(include=[np.number])\n",
    "    df_sub = df_sub.loc[:, df_sub.nunique() > 1]\n",
    "    if df_sub.shape[1] < 2:\n",
    "        print(f\"[{fs_name}] only {df_sub.shape[1]} varying feature(s), skipping.\\n\")\n",
    "        continue\n",
    "    X = df_sub.values.astype(float)\n",
    "\n",
    "    vif_results = []\n",
    "    for i, col in enumerate(df_sub.columns):\n",
    "        vif = variance_inflation_factor(X, i)\n",
    "        vif_results.append((col, vif))\n",
    "\n",
    "    vif_df = (\n",
    "        pd.DataFrame(vif_results, columns=[\"Feature\", \"VIF\"])\n",
    "          .sort_values(\"VIF\", ascending=False)\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    print(f\"\\n=== VIF for feature-set: {fs_name} (no intercept) ===\")\n",
    "    print(vif_df.to_string(index=False))\n"
   ],
   "id": "5746f2c2796c2b38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "perf_lookup  = results_df.set_index(['FeatureSet','Model','Fuel'])[['R2','MAE','RMSE']]\n",
    "fuels_list   = results_df['Fuel'].unique()\n",
    "cmap         = plt.cm.get_cmap('tab20', len(fuels_list))\n",
    "fuel_colors  = {fuel: cmap(i) for i, fuel in enumerate(fuels_list)}\n",
    "\n",
    "# Top-6 combos PARITY\n",
    "top6 = best.head(6)\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(20, 28))\n",
    "axes = axes.flatten()\n",
    "for ax, (_, row) in zip(axes, top6.iterrows()):\n",
    "    fs, model = row['FeatureSet'], row['Model']\n",
    "    bp = best_params_all[fs][model]\n",
    "\n",
    "    parts = []\n",
    "    for key, val in bp.items():\n",
    "        name = key.split('__',1)[1]\n",
    "        parts.append(f\"{name}={val:.4f}\" if isinstance(val, float) else f\"{name}={val}\")\n",
    "    param_str = \", \".join(parts)\n",
    "    sub = parity_df[(parity_df['FeatureSet']==fs) & (parity_df['Model']==model)]\n",
    "    if sub.empty:\n",
    "        continue\n",
    "\n",
    "    for fuel in fuels_list:\n",
    "        sub2 = sub[sub['Fuel']==fuel]\n",
    "        if sub2.empty:\n",
    "            continue\n",
    "        r2, mae, rmse = perf_lookup.loc[(fs,model,fuel)]\n",
    "        lbl = f\"{fuel} (R²={r2:.2f}, MAE={mae:.2f}, RMSE={rmse:.2f})\"\n",
    "        ax.scatter(sub2['True'], sub2['Predicted'],\n",
    "                   color=fuel_colors[fuel], alpha=0.6, label=lbl)\n",
    "\n",
    "    mn, mx = sub[['True','Predicted']].values.min(), sub[['True','Predicted']].values.max()\n",
    "    ax.plot([mn,mx],[mn,mx],'--',color='gray')\n",
    "\n",
    "    ax.set_title(f\"{model} | {fs}\\nBest params: {param_str}\", fontsize=12)\n",
    "    ax.set_xlabel('True Volatile Yield')\n",
    "    ax.set_ylabel('Predicted Volatile Yield')\n",
    "\n",
    "    ax.legend(\n",
    "        loc='lower center',\n",
    "        bbox_to_anchor=(0.5, -0.45),\n",
    "        bbox_transform=ax.transAxes,\n",
    "        ncol=2,\n",
    "        fontsize='medium',\n",
    "        frameon=True,\n",
    "        borderpad=0.5\n",
    "    )\n",
    "\n",
    "plt.subplots_adjust(hspace=0.6, wspace=0.27)\n",
    "plt.show()\n"
   ],
   "id": "de581885302dd59"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
